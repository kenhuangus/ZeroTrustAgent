Set up the SDK
This guide uses the google-genai Python SDK to connect to the Gemini 2.0 models.

Install SDK
The new Google Gen AI SDK provides programmatic access to Gemini 2 (and previous models) using both the Google AI for Developers and Vertex AI APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.

More details about this new SDK on the documentation or in the Getting started notebook.


!pip install -U -q google-genai
     
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/110.9 kB ? eta -:--:--
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 102.4/110.9 kB 7.1 MB/s eta 0:00:01
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.9/110.9 kB 2.6 MB/s eta 0:00:00
Set up your API key
To run the following cell, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the Authentication quickstart for an example.


import os
from google.colab import userdata

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')
     
Initialize SDK client
The client will pick up your API key from the environment variable. To use the live API you need to set the client version to v1alpha and use the Gemini 2.0 model.


from google import genai

client = genai.Client(http_options={'api_version': 'v1alpha'})

MODEL = 'gemini-2.0-flash-exp'
     
Use search in chat
Start by defining a helper function that you will use to display each part of the returned response.


# @title Define some helpers (run this cell)
import json

from IPython.display import display, HTML, Markdown


def show_json(obj):
  print(json.dumps(obj.model_dump(exclude_none=True), indent=2))

def show_parts(r):
  parts = r.candidates[0].content.parts
  if parts is None:
    finish_reason = r.candidates[0].finish_reason
    print(f'{finish_reason=}')
    return
  for part in r.candidates[0].content.parts:
    if part.text:
      display(Markdown(part.text))
    elif part.executable_code:
      display(Markdown(f'```python\n{part.executable_code.code}\n```'))
    else:
      show_json(part)

  grounding_metadata = r.candidates[0].grounding_metadata
  if grounding_metadata and grounding_metadata.search_entry_point:
    display(HTML(grounding_metadata.search_entry_point.rendered_content))

     
First try a query that needs realtime information, so you can see how the model performs without Google Search.


chat = client.chats.create(model=MODEL)

r = chat.send_message('Who won the most recent Australia vs Chinese Taipei games?')
show_parts(r)
     
To figure out who won the most recent game between Australia and Chinese Taipei, I need to know which sport you're referring to, as they compete in many different sports.

Please specify the sport you're interested in.

For example, are you asking about:

Soccer (Football)?
Basketball?
Baseball?
Other sport?
Once you tell me the sport, I can give you the correct information.

Now set up a new chat session that uses the google_search tool. The show_parts helper will display the text output as well as any Google Search queries used in the results.


search_tool = {'google_search': {}}
soccer_chat = client.chats.create(model=MODEL, config={'tools': [search_tool]})

r = soccer_chat.send_message('Who won the most recent Australia vs Chinese Taipei games?')
show_parts(r)
     
The most recent games between Australia and Chinese Taipei were in a friendly series in December 2024. The Australian women's national team, known as the Matildas, played against Chinese Taipei twice, winning both matches.

December 7, 2024: Australia won 6-0 against Chinese Taipei in Geelong. The goals were scored by Leah Davidson, Tameka Yallop, Emily Gielnik, Michelle Heyman, Tash Prior and Sharn Freier.
The first match of the series was also won by Australia, although the specific score is not detailed in the provided context.
Prior to these t